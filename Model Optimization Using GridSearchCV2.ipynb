{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0345af5c-cee2-4163-bd2b-7d292fe9e928",
   "metadata": {},
   "source": [
    "#Apply GridSearchCV for hyperparameter tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "60384d30-93c1-4513-8e6c-991aeff25313",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GridSearchCV with robust file loading (handles .xls that is actually CSV)\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "68da5f51-49a4-4009-9d66-3e2ae2081472",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------- Config: update folder if needed ----------\n",
    "folder = r\"C:\\Users\\dell\\Downloads\\project\"\n",
    "\n",
    "# List of possible dataset file names\n",
    "candidates = [\n",
    "    \"Cleaned_AI_Task_Data.xls\",\n",
    "    \"Cleaned_AI_Task_Data.xlsx\",\n",
    "    \"Cleaned_AI_Task_Data.csv\",\n",
    "    \"Cleaned_AI_Task_Data_NLP.xls\",\n",
    "    \"AI_Powered_Task_Management_System_2000.csv\"\n",
    "]\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "85f920b2-7e4c-4249-8a14-09b1797d8714",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using file: C:\\Users\\dell\\Downloads\\project\\Cleaned_AI_Task_Data.xls\n"
     ]
    }
   ],
   "source": [
    "# ---------- 1) find file ----------\n",
    "file_path = None\n",
    "for name in candidates:\n",
    "    p = os.path.join(folder, name)\n",
    "    if os.path.exists(p):\n",
    "        file_path = p\n",
    "        break\n",
    "\n",
    "# fallback: any file that contains the base name\n",
    "if file_path is None:\n",
    "    for f in os.listdir(folder):\n",
    "        if \"Cleaned_AI_Task_Data\" in f:\n",
    "            file_path = os.path.join(folder, f)\n",
    "            break\n",
    "\n",
    "if file_path is None:\n",
    "    raise FileNotFoundError(f\"No `Cleaned_AI_Task_Data.*` found in {folder}. Files: {os.listdir(folder)[:30]}\")\n",
    "\n",
    "print(\"Using file:\", file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "eb67db7c-2e04-417b-9526-b67ef61f8c73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded. Shape: (2000, 20)\n",
      "Columns: ['Task_ID', 'Title', 'Description', 'Assignee', 'Created_Date', 'Due_Date', 'Completed_Date', 'Status', 'Priority', 'Estimated_Hours', 'Actual_Hours', 'Project', 'Labels/Tags', 'Task_Complexity', 'Predicted_Priority', 'Delay_Risk_Score', 'Completion_Probability', 'AI_Recommendation', 'Task_Duration_Days', 'Actual_Duration_Days']\n"
     ]
    }
   ],
   "source": [
    "# ---------- 2) robust loader: read CSV if content looks like CSV even if extension is .xls ----------\n",
    "def robust_read(path):\n",
    "    ext = os.path.splitext(path)[1].lower()\n",
    "    # peek first bytes\n",
    "    try:\n",
    "        with open(path, \"rb\") as fh:\n",
    "            start = fh.read(4096)\n",
    "        text = start.decode(\"utf-8\", errors=\"ignore\")\n",
    "    except Exception:\n",
    "        text = \"\"\n",
    "\n",
    "    looks_like_csv = (\",\" in text and \"\\n\" in text and text.strip().splitlines()[0].count(\",\") >= 1)\n",
    "\n",
    "    if ext == \".csv\" or looks_like_csv:\n",
    "        # try csv read with fallback encodings\n",
    "        for enc in (\"utf-8\", \"latin1\", \"cp1252\"):\n",
    "            try:\n",
    "                return pd.read_csv(path, encoding=enc)\n",
    "            except Exception:\n",
    "                pass\n",
    "        raise RuntimeError(\"Failed to read file as CSV (tried utf-8, latin1, cp1252).\")\n",
    "    else:\n",
    "        # try xlsx then xls engines\n",
    "        try:\n",
    "            return pd.read_excel(path, engine=\"openpyxl\")\n",
    "        except Exception:\n",
    "            try:\n",
    "                # xlrd may not support some formats; attempt anyway\n",
    "                return pd.read_excel(path, engine=\"xlrd\")\n",
    "            except Exception:\n",
    "                # last resort, try as CSV\n",
    "                try:\n",
    "                    return pd.read_csv(path)\n",
    "                except Exception as e:\n",
    "                    raise RuntimeError(f\"Failed to read file as Excel or CSV: {e}\")\n",
    "\n",
    "# load\n",
    "df = robust_read(file_path)\n",
    "print(\"Loaded. Shape:\", df.shape)\n",
    "print(\"Columns:\", df.columns.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "d6a90907-5877-41ae-9e94-d4e7167d369b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Preview (first 5 rows):\n",
      "   Task_ID                      Title  \\\n",
      "0        1              Data Analysis   \n",
      "1        2                Backend API   \n",
      "2        3              Data Analysis   \n",
      "3        4              Data Analysis   \n",
      "4        5  User Feedback Integration   \n",
      "\n",
      "                                         Description assignee Created_Date  \\\n",
      "0     Documentation task for Data Analytics project.   George   2024-07-19   \n",
      "1       Backend API task for Data Analytics project.    Diana   2024-10-14   \n",
      "2  User Feedback Integration task for Smart Workf...   George   2024-06-18   \n",
      "3  User Feedback Integration task for Data Analyt...    Ethan   2024-06-13   \n",
      "4    Documentation task for AI Task Manager project.    Fiona   2024-04-06   \n",
      "\n",
      "     Due_Date Completed_Date     Status  priority  Estimated_Hours  \\\n",
      "0  2024-07-29     2024-07-29  Completed  Critical                3   \n",
      "1  2024-10-23     2024-10-23  Completed    Medium               20   \n",
      "2  2024-06-26     2024-06-26  Completed      High                9   \n",
      "3  2024-06-26            NaN      To Do      High                8   \n",
      "4  2024-04-17     2024-04-17  Completed  Critical               20   \n",
      "\n",
      "   Actual_Hours          Project Labels/Tags Task_Complexity  \\\n",
      "0             6   Data Analytics          ML          Medium   \n",
      "1            22  AI Task Manager         NLP            High   \n",
      "2             9  AI Task Manager     backend             Low   \n",
      "3            11   Data Analytics          AI             Low   \n",
      "4            20   Smart Workflow    frontend          Medium   \n",
      "\n",
      "  Predicted_Priority  Delay_Risk_Score  Completion_Probability  \\\n",
      "0                Low              0.54                    0.46   \n",
      "1             Medium              0.32                    0.68   \n",
      "2               High              0.30                    0.70   \n",
      "3           Critical              0.68                    0.32   \n",
      "4                Low              0.40                    0.60   \n",
      "\n",
      "                      AI_Recommendation  Task_Duration_Days  \\\n",
      "0  Maintain current workflow efficiency                  10   \n",
      "1        Reassign to experienced member                   9   \n",
      "2        Reassign to experienced member                   8   \n",
      "3              Review task dependencies                  13   \n",
      "4              Review task dependencies                  11   \n",
      "\n",
      "   Actual_Duration_Days  \n",
      "0                  10.0  \n",
      "1                   9.0  \n",
      "2                   8.0  \n",
      "3                   NaN  \n",
      "4                  11.0  \n"
     ]
    }
   ],
   "source": [
    "# ---------- 3) Ensure required columns exist (priority, assignee, Estimated_Hours) ----------\n",
    "# Normalize column names\n",
    "cols_lower = [c.lower().strip() for c in df.columns]\n",
    "col_map = {c: df.columns[i] for i, c in enumerate(cols_lower)}\n",
    "\n",
    "def find_col(possible_names):\n",
    "    for name in possible_names:\n",
    "        if name.lower() in col_map:\n",
    "            return col_map[name.lower()]\n",
    "    return None\n",
    "\n",
    "priority_col = find_col([\"priority\", \"priority_level\", \"task_priority\", \"urgency\"])\n",
    "assignee_col = find_col([\"assignee\", \"assigned_to\", \"assigned\", \"owner\", \"employee\"])\n",
    "est_hours_col = find_col([\"estimated_hours\", \"estimated hours\", \"est_hours\", \"estimated_hours\", \"estimated\"])\n",
    "\n",
    "# create or rename to standard names\n",
    "if priority_col:\n",
    "    df.rename(columns={priority_col: \"priority\"}, inplace=True)\n",
    "else:\n",
    "    if est_hours_col:\n",
    "        print(\"No priority column found — creating by Estimated_Hours heuristic.\")\n",
    "        df.rename(columns={est_hours_col: \"Estimated_Hours\"}, inplace=True)\n",
    "        df[\"priority\"] = pd.cut(df[\"Estimated_Hours\"].astype(float),\n",
    "                                bins=[-1, 3, 7, 1e9],\n",
    "                                labels=[\"Low\", \"Medium\", \"High\"]).astype(str)\n",
    "    else:\n",
    "        print(\"No priority or estimated-hours column found — creating synthetic priority.\")\n",
    "        np.random.seed(42)\n",
    "        df[\"priority\"] = np.random.choice([\"Low\", \"Medium\", \"High\"], size=len(df))\n",
    "\n",
    "if assignee_col:\n",
    "    df.rename(columns={assignee_col: \"assignee\"}, inplace=True)\n",
    "else:\n",
    "    print(\"No assignee column found — creating synthetic assignees for training.\")\n",
    "    np.random.seed(1)\n",
    "    df[\"assignee\"] = np.random.choice([\"Alice\",\"Bob\",\"Charlie\",\"David\"], size=len(df))\n",
    "\n",
    "if \"Estimated_Hours\" not in df.columns:\n",
    "    # try to find any numeric/time-like column; else create synthetic\n",
    "    possible_numeric = [c for c in df.columns if pd.api.types.is_numeric_dtype(df[c])]\n",
    "    if possible_numeric:\n",
    "        df.rename(columns={possible_numeric[0]: \"Estimated_Hours\"}, inplace=True)\n",
    "        print(f\" Renamed numeric column '{possible_numeric[0]}' to 'Estimated_Hours'.\")\n",
    "    else:\n",
    "        print(\"No numeric column found — creating synthetic Estimated_Hours.\")\n",
    "        np.random.seed(2)\n",
    "        df[\"Estimated_Hours\"] = np.random.randint(1, 10, size=len(df))\n",
    "\n",
    "# show head\n",
    "print(\"\\nPreview (first 5 rows):\")\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "fcfeb670-6ed9-4b4c-ace6-bb13f8ceb6f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------- 4) Prepare features & labels ----------\n",
    "# Encode priority and assignee\n",
    "priority_le = LabelEncoder()\n",
    "assignee_le = LabelEncoder()\n",
    "\n",
    "df[\"priority_enc\"] = priority_le.fit_transform(df[\"priority\"].astype(str))\n",
    "df[\"assignee_enc\"] = assignee_le.fit_transform(df[\"assignee\"].astype(str))\n",
    "\n",
    "X = df[[\"priority_enc\", \"Estimated_Hours\"]]\n",
    "y = df[\"assignee_enc\"]\n",
    "\n",
    "# If too few classes or samples, handle gracefully\n",
    "if len(df) < 5 or len(np.unique(y)) < 2:\n",
    "    raise ValueError(\"Not enough data/classes to run GridSearchCV. Need >=2 assignee classes and enough samples.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "89be6120-1f5f-4bb3-9c93-fefc54f5dbd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------- 5) Train/test split ----------\n",
    "stratify = y if len(np.unique(y)) > 1 and len(df) >= 10 else None\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=stratify)\n",
    "\n",
    "# ---------- 6) Define a reasonable parameter grid ----------\n",
    "param_grid = {\n",
    "    \"n_estimators\": [50, 100, 150],\n",
    "    \"max_depth\": [None, 6, 12],\n",
    "    \"min_samples_split\": [2, 5],\n",
    "    \"min_samples_leaf\": [1, 2],\n",
    "}\n",
    "\n",
    "rf = RandomForestClassifier(random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "61d856a0-1ec4-4c2f-a019-29a8043a6537",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Running GridSearchCV... (this can take some minutes depending on data size)\n",
      "Fitting 3 folds for each of 36 candidates, totalling 108 fits\n",
      "\n",
      " GridSearchCV complete.\n",
      "Best params: {'max_depth': 6, 'min_samples_leaf': 1, 'min_samples_split': 5, 'n_estimators': 100}\n"
     ]
    }
   ],
   "source": [
    "# ---------- 7) GridSearchCV (3-fold to save time) ----------\n",
    "grid = GridSearchCV(\n",
    "    estimator=rf,\n",
    "    param_grid=param_grid,\n",
    "    cv=3,\n",
    "    scoring=\"accuracy\",\n",
    "    n_jobs=-1,\n",
    "    verbose=2\n",
    ")\n",
    "\n",
    "print(\"\\nRunning GridSearchCV... (this can take some minutes depending on data size)\")\n",
    "grid.fit(X_train, y_train)\n",
    "\n",
    "print(\"\\n GridSearchCV complete.\")\n",
    "print(\"Best params:\", grid.best_params_)\n",
    "best_rf = grid.best_estimator_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "3c32457d-3e39-4d82-bc60-aa295a540151",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test Accuracy (best RF): 0.0850\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       Alice       0.06      0.04      0.04        53\n",
      "         Bob       0.02      0.02      0.02        47\n",
      "     Charlie       0.05      0.04      0.04        49\n",
      "       Diana       0.10      0.18      0.13        51\n",
      "       Ethan       0.10      0.08      0.09        52\n",
      "       Fiona       0.12      0.09      0.10        47\n",
      "      George       0.12      0.16      0.13        51\n",
      "      Hannah       0.11      0.08      0.09        50\n",
      "\n",
      "    accuracy                           0.09       400\n",
      "   macro avg       0.08      0.08      0.08       400\n",
      "weighted avg       0.08      0.09      0.08       400\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ---------- 8) Evaluate on test set ----------\n",
    "y_pred = best_rf.predict(X_test)\n",
    "acc = accuracy_score(y_test, y_pred)\n",
    "print(f\"\\nTest Accuracy (best RF): {acc:.4f}\")\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred, target_names=assignee_le.classes_))\n",
    "\n",
    "# Optionally save the tuned model\n",
    "# import joblib\n",
    "# joblib.dump(best_rf, \"best_rf_assignment_model.pkl\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
